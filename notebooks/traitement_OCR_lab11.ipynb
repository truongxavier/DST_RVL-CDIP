{"cells":[{"cell_type":"markdown","metadata":{"id":"bbvPQOLDfmYG"},"source":["# Traitemnt OCR label par label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":20704,"status":"ok","timestamp":1725084226289,"user":{"displayName":"xav tru","userId":"01413854554802189079"},"user_tz":-120},"id":"aRZStT27ftdq","outputId":"4abeaf13-bcae-47e7-d12c-1ba847f7febc"},"outputs":[],"source":["# Etape 2 :\n","# Installer les bibliothèques nécessaires\n","!pip install easyocr\n","!pip install tensorflow\n","!pip install datasets\n","!pip install nltk\n","!pip install pandas\n","!pip install opencv-python-headless  # Pour éviter les conflits avec cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265,"referenced_widgets":["777d00a2f9324613bf5019f68aeac5d1","443deae275614a80b28b94fd36085516","c94ee3579fd14bd7ab14436cc27b0184","4f4ad5b461014d27ab385c723edc3e2f","efbb6a46430a4b2883ebf02fc060e615","c27acf49987a480ab93288d25d8484e9","ee9e5e213206453d96abbe5dbc7eeb72","6c6a738877064b2fac86de0dc4436e85","bf7ef50c7ccc4258b5ddf364eb7294ba","c4701e67ab9440f3a67b03b9edcd6553","3a39bef5ed054be0b5357fd80279c8e5","311a4b26b8a64f408299915f5e2b3200","2245fc723a294a9ca52a44db785bf4cf","acf38154d9eb4f66b73731b801008a2f","2e6cb1b726554a9b8b3b023624be67e6","104a3d63c3844e39870aa8c3ec8abfbe","7657b167b3f740dc803dcc5b86d69765","3461176ac40b432cac207c737d2bbd95","d494a965a2e8487eaf4b4b638f1dbbff","53d51d164ab8415590d905b3ddde01a2","c5a0145722b44db88cd27f2638595e48","303c075995344310b7350328e2821195"]},"id":"__rZ2wU7fhRT","outputId":"e53c0892-f669-452d-cdf5-e158179375ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU configuré pour l'utilisation.\n","Aucun fichier CSV trouvé, création d'un nouveau DataFrame.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"777d00a2f9324613bf5019f68aeac5d1","version_major":2,"version_minor":0},"text/plain":["Loading dataset from disk:   0%|          | 0/53 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"311a4b26b8a64f408299915f5e2b3200","version_major":2,"version_minor":0},"text/plain":["Filtrage des images:   0%|          | 0/320000 [00:00<?, ?image/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Nombre d'images filtrées : 19947\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n","/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=device))\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n","  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"]}],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tqdm.notebook import tqdm\n","import cv2\n","import easyocr\n","import nltk\n","from PIL import Image, UnidentifiedImageError\n","from datasets import load_from_disk\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import io\n","import pandas as pd\n","import time\n","import threading\n","\n","#-------------------------------------------------------------------------------\n","#paramétrage de lancement\n","#-------------------------------------------------------------------------------\n","#chemin pour trouver les datasets en enregistrer le csv\n","results_dir = '/content/drive/MyDrive/formation Datascientest/RVL-CDIP/'\n","#nom du csv\n","csv_name ='ocr_results_label_'\n","#label cible sur lequel on va faire le calcul\n","LABEL_CIBLE = 11\n","#choix du datset val train ou test\n","type_dataset ='train'\n","# Nombre de threads pour le multithreading\n","num_threads = 24\n","#-------------------------------------------------------------------------------\n","#-------------------------------------------------------------------------------\n","\n","start_time = time.time()\n","# Create a lock\n","lock = threading.Lock()\n","\n","# Configurer TensorFlow pour utiliser le GPU\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","if len(physical_devices) > 0:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","    print(\"GPU configuré pour l'utilisation.\")\n","\n","# Chemin du fichier CSV pour sauvegarder les résultats OCR\n","csv_path = results_dir +csv_name + type_dataset + str(LABEL_CIBLE)+'.csv'\n","\n","# Charger le DataFrame existant si le fichier CSV existe\n","if os.path.exists(csv_path):\n","    df_ocr = pd.read_csv(csv_path)\n","    print(f\"Chargement du fichier CSV existant : {csv_path}\")\n","else:\n","    # Initialiser un DataFrame vide si le fichier n'existe pas\n","    df_ocr = pd.DataFrame(columns=['image_ID', 'texte_de_ocr', 'confiance_de_ocr', 'coordonnees_du_text',\n","                                   'coordonnees_des_zones_de_text', 'label_de_image'])\n","    print(\"Aucun fichier CSV trouvé, création d'un nouveau DataFrame.\")\n","\n","# Chargement des datasets\n","#train_dataset = load_from_disk(os.path.join(results_dir, 'train_dataset_ID_Resize'))\n","# test_dataset = load_from_disk(os.path.join(results_dir, 'test_dataset_ID_Resize'))\n","dataset = load_from_disk(os.path.join(results_dir, type_dataset+'_dataset_ID_Resize'))\n","\n","# Fonction pour filtrer les images du label\n","def filter_images(index):\n","    try:\n","        example = dataset[index]\n","        if example['label'] == LABEL_CIBLE:\n","            return example['image'], example['label'], example['image_ID']\n","    except Exception as e:\n","        print(f\"[WARNING] Erreur lors de la récupération de l'image à l'index {index}: {e}\")\n","    return None, None, None\n","\n","\n","# Filtrer les indices pour le traitement\n","indices = list(range(len(dataset)))\n","\n","# Utiliser ThreadPoolExecutor pour le multithreading avec une barre de progression\n","label_invoice_images = []\n","with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","    futures = [executor.submit(filter_images, i) for i in indices]\n","    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Filtrage des images\", unit=\"image\"):\n","        result, label, image_ID= future.result()\n","        if result is not None:\n","            label_invoice_images.append((result, label, image_ID))\n","\n","print(f\"Nombre d'images filtrées : {len(label_invoice_images)}\")\n","\n","# Fonction de prétraitement de l'image avec OpenCV\n","def preprocess_image(image_bytes):\n","    try:\n","        # Charger l'image avec PIL pour gérer le format .tif\n","        image = Image.open(io.BytesIO(image_bytes))\n","        image = image.convert('RGB')  # Convertir en RGB pour compatibilité OpenCV\n","\n","        # Convertir l'image en un tableau NumPy\n","        image_np = np.array(image)\n","\n","        # Redimensionnement\n","        #image_resized = cv2.resize(image_np, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_CUBIC)\n","        image_resized = image_np\n","\n","        # Conversion en niveaux de gris\n","        gray = cv2.cvtColor(image_resized, cv2.COLOR_RGB2GRAY)\n","\n","        # Seuillage adaptatif\n","        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n","\n","        # Suppression du bruit (médian)\n","        blur = cv2.medianBlur(thresh, 3)\n","\n","        # Encoder l'image traitée en bytes pour OCR\n","        _, buffer = cv2.imencode('.png', blur)\n","        preprocessed_image_bytes = buffer.tobytes()\n","\n","        return preprocessed_image_bytes\n","\n","    except Exception as e:\n","        print(f\"Erreur de prétraitement sur l'image: {e}\")\n","        return None\n","\n","# Initialiser le lecteur EasyOCR\n","reader = easyocr.Reader(['en'], gpu=True)\n","\n","# Fonction pour appliquer l'OCR avec EasyOCR\n","def apply_ocr(image_bytes):\n","    try:\n","        # Convertir les bytes en une image pour EasyOCR\n","        image_np = np.array(Image.open(io.BytesIO(image_bytes)))\n","        result = reader.readtext(image_np, detail=1)\n","\n","        # Préparer les données à retourner\n","        texts = []\n","        confidences = []\n","        coordinates = []\n","        for (coord, text, confidence) in result:\n","            texts.append(text)\n","            confidences.append(confidence)\n","            coordinates.append(coord)\n","\n","        return texts, confidences, coordinates\n","\n","    except UnidentifiedImageError:\n","        print(f\"[WARNING] Impossible d'identifier le fichier image. Ignorer...\")\n","        return [], [], []\n","    except Exception as e:\n","        print(f\"[ERROR] Erreur lors de l'application de l'OCR: {e}\")\n","        return [], [], []\n","\n","# Liste pour stocker les résultats\n","ocr_results = []\n","\n","# Charger les noms d'image déjà traités pour éviter les doublons\n","images_deja_traitees = set(df_ocr['image_ID'])\n","\n","# Appliquer le prétraitement et l'OCR sur toutes les images filtrées après prétraitement avec multithreading\n","with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","    futures = {}\n","    for image, label, image_ID in label_invoice_images:\n","        # ID de l'image\n","        image_name = image_ID\n","\n","        # Vérifier si l'image a déjà été traitée\n","        if image_name in images_deja_traitees:\n","            #print(f\"Image {image_name} déjà traitée, passage...\")\n","            continue\n","\n","        # Convertir l'image en bytes directement si ce n'est pas déjà le cas\n","        if isinstance(image, Image.Image):\n","            image_bytes = io.BytesIO()\n","            image.save(image_bytes, format='TIFF')\n","            image_bytes = image_bytes.getvalue()\n","        else:\n","            image_bytes = image\n","\n","        preprocessed_image_bytes = preprocess_image(image_bytes)\n","        if preprocessed_image_bytes:\n","            future = executor.submit(apply_ocr, preprocessed_image_bytes)\n","            futures[future] = (image_name, label, preprocessed_image_bytes)\n","\n","    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Prétraitement et OCR des images\", unit=\"image\"):\n","        image_name, label, preprocessed_image_bytes = futures[future]\n","        texts, confidences, coordinates = future.result()\n","\n","        # Enregistrer les résultats dans la liste\n","        ocr_results.append({\n","            'image_ID': image_name,\n","            'texte_de_ocr': \" \".join(texts),\n","            'confiance_de_ocr': confidences,\n","            'coordonnees_du_text': coordinates,\n","            'coordonnees_des_zones_de_text': [coordinates],  # Peut être la même que les coordonnées du texte dans ce contexte\n","            'label_de_image': label\n","        })\n","\n","        # Sauvegarder les résultats dans le DataFrame existant et enregistrer le fichier CSV\n","        with lock:\n","          new_row = pd.DataFrame([{\n","              'image_ID': image_name,\n","              'texte_de_ocr': \" \".join(texts),\n","              'confiance_de_ocr': confidences,\n","              'coordonnees_du_text': coordinates,\n","              'coordonnees_des_zones_de_text': [coordinates],  # Peut être la même que les coordonnées du texte dans ce contexte\n","              'label_de_image': label\n","          }])\n","          df_ocr = pd.concat([df_ocr, new_row], ignore_index=True)\n","          df_ocr.to_csv(csv_path, index=False)\n","          #print(f\"Résultats sauvegardés pour {image_name}.\")\n","\n","print(\"Tous les résultats de l'OCR ont été enregistrés dans \"+ csv_path)\n","end_time = time.time()\n","processing_time = end_time - start_time\n","print(f\"Temps de traitement total : {round(processing_time / 60, 2)} minutes\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMNthtvC8pu8PdHWSn3fEv8","gpuType":"T4","machine_shape":"hm","mount_file_id":"1em1-z1_y1bj0ljfTmA6n-uTKP4eWNCmR","provenance":[{"file_id":"1gyx1LO7WARDQw-Bg_uKVcFYM4NuwJ9WE","timestamp":1725052917438}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"104a3d63c3844e39870aa8c3ec8abfbe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2245fc723a294a9ca52a44db785bf4cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7657b167b3f740dc803dcc5b86d69765","placeholder":"​","style":"IPY_MODEL_3461176ac40b432cac207c737d2bbd95","value":"Filtrage des images: 100%"}},"2e6cb1b726554a9b8b3b023624be67e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5a0145722b44db88cd27f2638595e48","placeholder":"​","style":"IPY_MODEL_303c075995344310b7350328e2821195","value":" 320000/320000 [00:15&lt;00:00, 3193.13image/s]"}},"303c075995344310b7350328e2821195":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"311a4b26b8a64f408299915f5e2b3200":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2245fc723a294a9ca52a44db785bf4cf","IPY_MODEL_acf38154d9eb4f66b73731b801008a2f","IPY_MODEL_2e6cb1b726554a9b8b3b023624be67e6"],"layout":"IPY_MODEL_104a3d63c3844e39870aa8c3ec8abfbe"}},"3461176ac40b432cac207c737d2bbd95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a39bef5ed054be0b5357fd80279c8e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"443deae275614a80b28b94fd36085516":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c27acf49987a480ab93288d25d8484e9","placeholder":"​","style":"IPY_MODEL_ee9e5e213206453d96abbe5dbc7eeb72","value":"Loading dataset from disk: 100%"}},"4f4ad5b461014d27ab385c723edc3e2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4701e67ab9440f3a67b03b9edcd6553","placeholder":"​","style":"IPY_MODEL_3a39bef5ed054be0b5357fd80279c8e5","value":" 53/53 [00:00&lt;00:00, 48.56it/s]"}},"53d51d164ab8415590d905b3ddde01a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c6a738877064b2fac86de0dc4436e85":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7657b167b3f740dc803dcc5b86d69765":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"777d00a2f9324613bf5019f68aeac5d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_443deae275614a80b28b94fd36085516","IPY_MODEL_c94ee3579fd14bd7ab14436cc27b0184","IPY_MODEL_4f4ad5b461014d27ab385c723edc3e2f"],"layout":"IPY_MODEL_efbb6a46430a4b2883ebf02fc060e615"}},"acf38154d9eb4f66b73731b801008a2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d494a965a2e8487eaf4b4b638f1dbbff","max":320000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53d51d164ab8415590d905b3ddde01a2","value":320000}},"bf7ef50c7ccc4258b5ddf364eb7294ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c27acf49987a480ab93288d25d8484e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4701e67ab9440f3a67b03b9edcd6553":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5a0145722b44db88cd27f2638595e48":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c94ee3579fd14bd7ab14436cc27b0184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c6a738877064b2fac86de0dc4436e85","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf7ef50c7ccc4258b5ddf364eb7294ba","value":53}},"d494a965a2e8487eaf4b4b638f1dbbff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee9e5e213206453d96abbe5dbc7eeb72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efbb6a46430a4b2883ebf02fc060e615":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
