{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation BERT pour classification des images par leur OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'OCR avec EasyOCR\n",
    "!pip install easyocr\n",
    "\n",
    "# Pour les opérations avec PyTorch (déjà inclus dans Google Colab, mais si vous êtes sur un environnement local)\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Pour utiliser Hugging Face Transformers (pour le modèle BERT et le tokenizer)\n",
    "!pip install transformers\n",
    "\n",
    "# Pour manipuler des datasets avec Hugging Face (facultatif mais utile pour gérer les datasets)\n",
    "!pip install datasets\n",
    "\n",
    "# Pour travailler avec des DataFrames (par exemple pour `df_ocr`)\n",
    "!pip install pandas\n",
    "\n",
    "# Pour la division des jeux de données (train/test split)\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "#paramétrage de lancement\n",
    "#-------------------------------------------------------------------------------\n",
    "results_dir = '/content/drive/MyDrive/formation Datascientest/RVL-CDIP/'\n",
    "csv_name = 'processed_ocr_results_reducedtrain.csv'\n",
    "#-------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------\n",
    "df_ocr = pd.read_csv(results_dir + csv_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement du texte pour le pipeline NLP\n",
    "Nous allons utiliser le tokenizer BERT pour préparer les données textuelles contenues dans la colonne texte_de_ocr de votre DataFrame. Voici comment effectuer le prétraitement des données.\n",
    "\n",
    "Charger le tokenizer BERT.\n",
    "Appliquer le tokenizer sur chaque texte de la DataFrame.\n",
    "Préparer les données pour être compatibles avec un modèle BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer BERT pré-entraîné\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Fonction pour tokeniser le texte\n",
    "def preprocess_texts(texts):\n",
    "    # Tokeniser tous les textes dans un seul batch\n",
    "    tokens = tokenizer(\n",
    "        texts.tolist(),  # Convertir la colonne en liste\n",
    "        padding='max_length',  # Remplir les séquences pour qu'elles aient toutes la même longueur\n",
    "        truncation=True,       # Troncation pour respecter la longueur maximale\n",
    "        max_length=512,        # Longueur maximale compatible avec BERT\n",
    "        return_tensors='pt'    # Retourner les tenseurs PyTorch\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "# Appliquer le prétraitement sur le texte de la DataFrame\n",
    "tokens = preprocess_texts(df_ocr['texte_de_ocr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des documents avec BERT\n",
    "Maintenant que nous avons prétraité les textes, nous allons construire un modèle de classification basé sur BERT. Le modèle va apprendre à prédire les labels des documents à partir des textes extraits.\n",
    "\n",
    "Charger un modèle BERT pré-entraîné pour la classification.\n",
    "Préparer les données pour l'entraînement (input_ids, attention_masks, labels).\n",
    "Entraîner et évaluer le modèle.\n",
    "5.1. Préparation des données pour l'entraînement\n",
    "Nous devons associer les textes tokenisés à leurs labels et préparer les tenseurs pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Préparer un dataset personnalisé pour PyTorch\n",
    "class RVLCDIPDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.input_ids = tokens['input_ids']\n",
    "        self.attention_mask = tokens['attention_mask']\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Créer le dataset à partir des tokens et des labels dans la DataFrame\n",
    "dataset = RVLCDIPDataset(tokens, df_ocr['label_de_image'].tolist())\n",
    "\n",
    "# Diviser en train et test (80% train, 20% test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_indices, test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer des DataLoader pour l'entraînement et l'évaluation\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du modèle BERT pour la classification\n",
    "Nous allons maintenant charger un modèle BERT pour la classification de séquences et l'entraîner sur le dataset tokenisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Charger le modèle BERT pré-entraîné pour la classification avec 16 labels\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=16)\n",
    "\n",
    "# Préparer les arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Répertoire de sortie\n",
    "    evaluation_strategy=\"epoch\",     # Évaluer à chaque époque\n",
    "    per_device_train_batch_size=16,  # Taille de batch pour l'entraînement\n",
    "    per_device_eval_batch_size=16,   # Taille de batch pour l'évaluation\n",
    "    num_train_epochs=3,              # Nombre d'époques d'entraînement\n",
    "    weight_decay=0.01,               # Taux de régularisation\n",
    "    logging_dir='./logs',            # Répertoire des logs\n",
    ")\n",
    "\n",
    "# Créer un objet Trainer pour gérer l'entraînement\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "trainer.train()\n",
    "\n",
    "# Évaluer le modèle\n",
    "trainer.evaluate()\n",
    "\n",
    "# Effectuer l'évaluation\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier texte\n",
    "with open(\"evaluation_results.txt\", \"w\") as f:\n",
    "    for key, value in results.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(\"Les résultats d'évaluation ont été sauvegardés dans 'evaluation_results.txt'.\")\n",
    "\n",
    "import json\n",
    "# Sauvegarder les résultats dans un fichier JSON\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Les résultats d'évaluation ont été sauvegardés dans 'evaluation_results.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Spécifier le répertoire où enregistrer le modèle\n",
    "output_dir = 'content/saved_model/'\n",
    "\n",
    "# Vérifier si le répertoire existe, sinon le créer\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Répertoire {output_dir} créé avec succès.\")\n",
    "else:\n",
    "    print(f\"Le répertoire {output_dir} existe déjà.\")\n",
    "\n",
    "# Sauvegarder le modèle entraîné\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Sauvegarder le tokenizer utilisé pour la tokenisation\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Le modèle et le tokenizer ont été sauvegardés dans le répertoire {output_dir}\")\n",
    "\n",
    "drive_model_dir = '/content/drive/MyDrive/formation Datascientest/jul24_bds_extraction/ETAPE 2/saved_model/'\n",
    "# Copier le répertoire local vers Google Drive\n",
    "shutil.copytree(output_dir, drive_model_dir, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"Le répertoire {output_dir} a été copié sur Google Drive à {drive_model_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rechargement du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Recharger le modèle à partir du répertoire\n",
    "model = BertForSequenceClassification.from_pretrained(drive_model_dir)\n",
    "\n",
    "# Recharger le tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(drive_model_dir)\n",
    "\n",
    "print(\"Le modèle et le tokenizer ont été rechargés avec succès.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
